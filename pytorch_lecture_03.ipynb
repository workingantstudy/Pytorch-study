{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSDPbkOb0iYCxYrQKerI4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/workingantstudy/Pytorch-study/blob/main/pytorch_lecture_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e82ks-thtzef",
        "outputId": "b443b397-6e73-4864-ed17-860d1c241c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st training\n",
            "\n",
            "\n",
            "w = 1\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.0 1.0   -2.0\n",
            "2.0 4.0 2.0 4.0   -8.0\n",
            "3.0 6.0 3.0 9.0   -18.0\n",
            "\n",
            "\n",
            "avgloss = 4.67 \t avggrad =  -9.33\n",
            "-------------------------------------------------------\n",
            "2nd training\n",
            "\n",
            "\n",
            "w = 1.47\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.47 0.28   -1.07\n",
            "2.0 4.0 2.93 1.14   -4.27\n",
            "3.0 6.0 4.4 2.56   -9.6\n",
            "\n",
            "\n",
            "avgloss = 1.33 \t avggrad =  -4.98\n",
            "-------------------------------------------------------\n",
            "3th training\n",
            "\n",
            "\n",
            "w = 1.72\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.72 0.08   -0.57\n",
            "2.0 4.0 3.43 0.32   -2.28\n",
            "3.0 6.0 5.15 0.73   -5.12\n",
            "\n",
            "\n",
            "avgloss = 0.38 \t avggrad =  -2.66\n",
            "-------------------------------------------------------\n",
            "4th training\n",
            "\n",
            "\n",
            "w = 1.85\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.85 0.02   -0.3\n",
            "2.0 4.0 3.7 0.09   -1.21\n",
            "3.0 6.0 5.55 0.21   -2.73\n",
            "\n",
            "\n",
            "avgloss = 0.11 \t avggrad =  -1.41\n",
            "-------------------------------------------------------\n",
            "5th training\n",
            "\n",
            "\n",
            "w = 1.92\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.92 0.01   -0.16\n",
            "2.0 4.0 3.84 0.03   -0.65\n",
            "3.0 6.0 5.76 0.06   -1.46\n",
            "\n",
            "\n",
            "avgloss = 0.03 \t avggrad =  -0.76\n",
            "-------------------------------------------------------\n",
            "6th training\n",
            "\n",
            "\n",
            "w = 1.96\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.96 0.0   -0.09\n",
            "2.0 4.0 3.91 0.01   -0.34\n",
            "3.0 6.0 5.87 0.02   -0.77\n",
            "\n",
            "\n",
            "avgloss = 0.01 \t avggrad =  -0.4\n",
            "-------------------------------------------------------\n",
            "7th training\n",
            "\n",
            "\n",
            "w = 1.98\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.98 0.0   -0.05\n",
            "2.0 4.0 3.95 0.0   -0.18\n",
            "3.0 6.0 5.93 0.0   -0.41\n",
            "\n",
            "\n",
            "avgloss = 0.0 \t avggrad =  -0.21\n",
            "-------------------------------------------------------\n",
            "8th training\n",
            "\n",
            "\n",
            "w = 1.99\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.99 0.0   -0.02\n",
            "2.0 4.0 3.98 0.0   -0.1\n",
            "3.0 6.0 5.96 0.0   -0.22\n",
            "\n",
            "\n",
            "avgloss = 0.0 \t avggrad =  -0.12\n",
            "-------------------------------------------------------\n",
            "9th training\n",
            "\n",
            "\n",
            "w = 1.99\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 1.99 0.0   -0.01\n",
            "2.0 4.0 3.99 0.0   -0.05\n",
            "3.0 6.0 5.98 0.0   -0.12\n",
            "\n",
            "\n",
            "avgloss = 0.0 \t avggrad =  -0.06\n",
            "-------------------------------------------------------\n",
            "10th training\n",
            "\n",
            "\n",
            "w = 2.0\n",
            "\n",
            "\n",
            " x   y   y^ loss  gradient\n",
            "1.0 2.0 2.0 0.0   -0.01\n",
            "2.0 4.0 3.99 0.0   -0.03\n",
            "3.0 6.0 5.99 0.0   -0.06\n",
            "\n",
            "\n",
            "avgloss = 0.0 \t avggrad =  -0.03\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#numpy, matplotlib.pyplot 라이브러리를 np와 plt로 입력\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#실제 라인 값 x y\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "#w=1 #무작위 값 w\n",
        "\n",
        "#linear한 함수 정의 -> forward(3)이면 3w의 값이 나옴\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "#loss 함수 정의 { 예측한 y와 실제 y의 거리차의 제곱 } -> loss(2,3)이면 (2w-3)^2의 값이 나옴\n",
        "def loss(x,y):\n",
        "  y_prediction = forward(x)\n",
        "  return (y_prediction-y)**2\n",
        "\n",
        "#gradient 함수 정의 : loss함수의 기울기로 +값이라면 w를 줄여야 하고, -값이면 늘려야 함\n",
        "def gradient(x,y):\n",
        "  return 2*x*(w*x-y)\n",
        "\n",
        "w=1 #랜덤한 값 w. 유의미한 코드를 위해 2를 제외한 랜덤한 수를 넣으면 됌.\n",
        "\n",
        "for epoch in range(10) : # 아래 코드를 10번동안 학습 (epoch 값은 0부터 9로 10번임)\n",
        "  loss_sum = 0 # 처음에 0이라고 정의해줘야 작동함.\n",
        "  grad_sum = 0\n",
        "\n",
        "  #기분내기용 if문\n",
        "  if epoch == 0 :\n",
        "    print(\"1st training\")\n",
        "  elif epoch == 1 :\n",
        "    print(\"2nd training\")\n",
        "  else :\n",
        "    print(f\"{epoch+1}th training\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"w =\", round(w, 2)) # w값부터 보여주고 시작하기\n",
        "  print(\"\\n\")\n",
        "  print(\" x   y   y^ loss  gradient\")\n",
        "\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val) #loss값을 l로 설정\n",
        "    loss_sum += l\n",
        "    grad = gradient(x_val, y_val) #gradient값을 grad로 설정\n",
        "    grad_sum += grad\n",
        "\n",
        "    print(x_val, y_val, round(forward(x_val), 2), round(l, 2), \" \", round(grad, 2))\n",
        "  avggrad = round(grad_sum/3, 2) #소수점 두자리까지 반올림\n",
        "  print(\"\\n\")\n",
        "  print(\"avgloss =\", round(loss_sum/3, 2),\"\\t\", \"avggrad = \", avggrad)\n",
        "\n",
        "  w = w - 0.05*avggrad   #w값 보정식\n",
        "  print(\"-------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}